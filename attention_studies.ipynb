{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9151f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d6759a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'baseline_11_29/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c171c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from capture24.patch_tst import PatchTST\n",
    "import yaml\n",
    "\n",
    "with open('capture24/config_patchtst.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['hook_attention_maps'] = True\n",
    "model = PatchTST(config)\n",
    "model.load_state_dict(torch.load(MODEL_PATH + 'patchtst_model.pth', weights_only=True, map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37e6b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Test Data \n",
    "with gzip.open('capture24/final_data_512/X_test.npy.gz', 'rb') as f:\n",
    "    X_test = np.load(f)\n",
    "\n",
    "with gzip.open('capture24/final_data_512/Y_test.npy.gz', 'rb') as f:\n",
    "    Y_test = np.load(f)\n",
    "\n",
    "with open('capture24/final_data_512/label_to_index.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "idx_to_label = data['index_to_label']\n",
    "label_to_idx = data['label_to_index']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(test_example, label, path = None):\n",
    "    # size will be (512, 3) -> (time, channels)\n",
    "    # conver to pandas dataframe\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            'time': range(test_example.shape[0]),\n",
    "            'channel_1': test_example[:, 0],\n",
    "            'channel_2': test_example[:, 1],\n",
    "            'channel_3': test_example[:, 2]\n",
    "        }\n",
    "    )\n",
    "    # plot the dataframe\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.lineplot(x = 'time', y = 'channel_1', data = df)\n",
    "    sns.lineplot(x = 'time', y = 'channel_2', data = df)\n",
    "    sns.lineplot(x = 'time', y = 'channel_3', data = df)\n",
    "    plt.title(f'Test Example for {label}')\n",
    "    if path: \n",
    "        plt.savefig(path)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from capture24.patch_tst import C24_Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# Identify a single example per class\n",
    "\n",
    "CLASS_NAME = list(label_to_idx.keys())[9]\n",
    "class_idx = label_to_idx[CLASS_NAME]\n",
    "\n",
    "\n",
    "# Find a single test example for this class\n",
    "test_idx = np.where(Y_test == class_idx)[0][0]\n",
    "\n",
    "# create folder for specific class\n",
    "path = MODEL_PATH + f'attention/{CLASS_NAME}/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Get the test example\n",
    "test_example = X_test[test_idx]\n",
    "plot_example(test_example, CLASS_NAME, path + f'{CLASS_NAME}.png')\n",
    "\n",
    "# Inference # add batch dimension\n",
    "test_example = test_example[np.newaxis, :, :]\n",
    "ex = C24_Dataset(test_example, np.array([Y_test[test_idx]]), idx_to_label, label_to_idx)\n",
    "loader = DataLoader(ex, batch_size=1, shuffle=False)\n",
    "\n",
    "for x, y in loader:\n",
    "    outputs = model(x)\n",
    "\n",
    "maps = model.attention_maps\n",
    "print(maps[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81244067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all of the attention maps\n",
    "for i, map in enumerate(maps):\n",
    "    path = MODEL_PATH + f'attention/{CLASS_NAME}/{i+1}/'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(MODEL_PATH + f'attention/{CLASS_NAME}/{i+1}')\n",
    "    for j, head in enumerate(map.squeeze()):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        sns.heatmap(head.cpu().numpy().squeeze(), cmap='viridis')\n",
    "        plt.title(f'Attention Map for layer{i+1} and head {j+1}')\n",
    "        # save plot to path folder\n",
    "        plt.savefig(path + f'layer_{i+1}_head_{j+1}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1357d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# For each activity, load all attention maps into a layers x heads grid, and save as a compiled image\n",
    "\n",
    "ACTIVITIES = list(label_to_idx.keys())\n",
    "NUM_LAYERS = 3\n",
    "NUM_HEADS = 16  # assumes shape [batch, heads, ...] as before\n",
    "\n",
    "for act in ACTIVITIES:\n",
    "    grid_images = []\n",
    "    for head in range(1, NUM_HEADS+1):\n",
    "        row_imgs = []\n",
    "        for layer in range(1, NUM_LAYERS+1):\n",
    "            fname = MODEL_PATH + f\"attention/{act}/{layer}/layer_{layer}_head_{head}.png\"\n",
    "            if os.path.exists(fname):\n",
    "                img = mpimg.imread(fname)\n",
    "            else:\n",
    "                # create a blank placeholder if it doesn't exist\n",
    "                img = np.ones((224, 224, 3), dtype=np.float32)  # Assuming 224x224 images, adjust as needed\n",
    "            row_imgs.append(img)\n",
    "        # horizontally stack for each row (head)\n",
    "        row_imgs = [img if img is not None else np.ones_like(row_imgs[0]) for img in row_imgs]\n",
    "        grid_images.append(np.concatenate(row_imgs, axis=1))\n",
    "    # vertically stack for all rows (heads)\n",
    "    grid_img = np.concatenate(grid_images, axis=0)\n",
    "    plt.figure(figsize=(NUM_LAYERS * 3, NUM_HEADS * 3))\n",
    "    plt.imshow(grid_img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"All Attention Maps for {act}\")\n",
    "    save_path = MODEL_PATH + f'attention/{act}_all_attention_grids.png'\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Coursework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
