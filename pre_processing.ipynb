{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a1b669",
   "metadata": {},
   "source": [
    "### Transformer Training Preprocessing Steps\n",
    "1. load all matrices in from prepared_data/\n",
    "2. convert annotations to HAC label standard to be selected\n",
    "3. Split into Train-Validation-Test -> using split mentioned in paper\n",
    "4. Save all into .npys in in specified directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "file_path = 'capture24/prepared_data_512'\n",
    "# Let's take a look at the prepared data\n",
    "X_from_npy = np.load(f'{file_path}/X.npy')\n",
    "print(X_from_npy[10])\n",
    "\n",
    "# Let's take a look at the labels\n",
    "Y_from_npy = np.load(f'{file_path}/Y_anno.npy', allow_pickle=True)\n",
    "print(Y_from_npy[10])\n",
    "\n",
    "Y_W2020 = np.load(f'{file_path}/Y_Walmsley2020.npy')\n",
    "print(Y_W2020[10])\n",
    "\n",
    "Y_Willetts2018 = np.load(f'{file_path}/Y_WillettsSpecific2018.npy')\n",
    "print(Y_Willetts2018[10])\n",
    "\n",
    "# Let's take a look at the time stamps\n",
    "T_from_npy = np.load(f'{file_path}/T.npy')\n",
    "print(T_from_npy[:10])\n",
    "\n",
    "# Let's take a look at the patient ids\n",
    "P_from_npy = np.load(f'{file_path}/P.npy')\n",
    "# Process data to remove P and convert to int\n",
    "P_from_npy = [int(i.split('P')[1]) for i in P_from_npy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training - Test Split\n",
    "# Capture 24 specified split as the following:\n",
    "# Participants 001 - 100 -> Training - for final (001-080: Train, 081-100: Validation)\n",
    "# Participants (P)101 - 150 -> Test\n",
    "\n",
    "# Easiest way to split is perhaps utilize a pandas dataframe for filtering\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Accelerometer_data\": [x for x in X_from_npy],\n",
    "        \"Raw_Labels\": Y_from_npy,\n",
    "        \"Walmsley2020_Labels\": Y_W2020,\n",
    "        \"Willetts2018_Labels\": Y_Willetts2018,\n",
    "        \"Time_Stamps\": T_from_npy,\n",
    "        \"Patient_ID\": P_from_npy\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Split into training and test - save under capture24/final_data/\n",
    "train_df = df[(df['Patient_ID'] <= 100)].reset_index(drop=True)\n",
    "test_df = df[(df['Patient_ID'] > 100)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8430e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Create Index Mapping for the Labels\n",
    "unique_labels = sorted(set(train_df['Willetts2018_Labels'].unique()))\n",
    "label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
    "index_to_label = {index: label for label, index in label_to_index.items()}\n",
    "\n",
    "\n",
    "# Encode the Labels - let's use the Willetts2018_Labels - in numpy format\n",
    "Y_train = np.array(train_df['Willetts2018_Labels'])\n",
    "Y_train_id = np.array([label_to_index[label] for label in Y_train])\n",
    "\n",
    "# Encode the test labels in a similar manner\n",
    "Y_test = np.array(test_df['Willetts2018_Labels'])\n",
    "Y_test_id = np.array([label_to_index[label] for label in Y_test])\n",
    "\n",
    "dir_path = 'capture24/final_data_512/'\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "with open(f'{dir_path}/label_to_index.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"label_to_index\": label_to_index,\n",
    "        \"index_to_label\": index_to_label\n",
    "    }, f)\n",
    "\n",
    "\n",
    "# Reshape the input data to be [B, T, C]\n",
    "X_train = train_df[\"Accelerometer_data\"].apply(\n",
    "    lambda x: np.asarray(x, dtype=np.float32)\n",
    ")\n",
    "\n",
    "X_test = test_df[\"Accelerometer_data\"].apply(\n",
    "    lambda x: np.asarray(x, dtype=np.float32)\n",
    ")\n",
    "\n",
    "\n",
    "np.save(f'{dir_path}/X_train.npy', np.stack(X_train.values))\n",
    "np.save(f'{dir_path}/X_test.npy', np.stack(X_test.values))\n",
    "\n",
    "# Save the labels\n",
    "np.save(f'{dir_path}/Y_train.npy', Y_train_id)\n",
    "np.save(f'{dir_path}/Y_test.npy', Y_test_id)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
